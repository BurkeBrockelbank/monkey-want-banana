TESTING LINEAR MODEL AGAINST AI
An AI was developed to navigate the monkey game. This AI was then used to generate over 1.3 million turns of gameplay. A linear model was trained with this data. Reinforcement learning was attempted after this, but no increase in the accuracy of the quality function was found so this training was reverted.

The models were scored against eachother based on the amount of food they could collect in 30 turns averaged over 1000 random placements. This was repeated five times each.

 Model | Score
-------+-------
Linear |  6.45, 6.923, 6.739, 7.158, 6.906
    AI | 39.341, 38.701, 39.334, 40.7, 39.474

It is clear that the linear model for the monkey brain is insufficiently complex for modelling the artificial intelligence.

PROGRESSION TO A FULL CNN
I added a convolutional layer to the progressional brain. The net architecture is
(4x11x11) & 1
(4x11x11) & 1 Conv2d ReLU
485 Flatten
9 Relu
8 Relu
5

It behaves quite well after training on AIDATA\AIData0.txt for 30 epochs in 3 batches each with randomized order between each epoch. This is in contrast to more rudimentary nets. Clearly the convolutional layer makes the network more capable of learning the AI algorithm.

Built gif of performance in ./img/OneConvLayer showing this. The qualities are clearly wrong, but at least the net is making the right decisions. After reinforcement elarning the qualities improve drastically. I will add in more layers.

Added yet another fully-connected layer
(4x11x11) & 1
(4x11x11) Conv2d ReLU & 1
484 Flatten & 1
25 ReLU & 1
26
9 Relu
8 Relu
5

This seems to be important. The supervised training is going a lot better with this layer
added in! The loss is looking to stagnate around 0.45 whereas without this layer it was getting stuck around 0.7.

The result of supervised training actually wasn't too good because the AI often gets stuck, the monkey often just got stuck itself.

This bad result carried over to the curated training. I will try again with fewer epochs.

Did 10 epochs and the loss stopped at 0.67 like the previous successful test. It still didn't work too well. The performance is much better. Adding in the rest of the convolutional layers.

(4x11x11) & 1
(4x11x11) Conv2d ReLU & 1
(6x9x9) Conv2d ReLU & 1
(4x7x7) Conv2d ReLU & 1
(1x5x5) Conv2d ReLU & 1
25 Flatten & 1
26 Concat
9 Relu
8 Relu
5

Supervised training started with higher than expected loss (2.95). Each epoch is now taking less time, but the training rate has decreased and stagnated at a loss of 1.2. Monkey behavior seems fairly random. The monkey is not making intelligent choices. This is not a surprise because the amount of training data was not increased. We will add in another two data files.

Using AIDATA\AIData0.txt, AIDATA\AIData1.txt, AIDATA\AIData2.txt for training. Supervised training seemed totally ineffective. The monkey just sits still. Not staying still anymore. I am going to increase the exploration rate to a constant 0.8 and go for another round of curated reinforcement learning.

The net seems to be insensitive to training.

BRAIN V3: SMALL SCALE UNLEARNABILITY
One of the reasons that the net was unable to learn was that it had too many convolutional layers, leading to a receptive field that is far larger than what we want. The monkey needs to be able to see its near surroundings exactly. As such, a separate branch will be created for this.

Convolutions are specified as (kernel size, padding, stride)

vision                  & vision                        & food
(4x11x11)               & (4x11x11)                     & 1
(4x5x5) Crop            & (8x11x11) Conv2d(3,1,1) ReLU  & 1
(4x5x5)                 & (4x8x8) Conv2d(4,0,1) ReLU    & 1
(4x5x5)                 & (2x5x5) Conv2d(4,0,1) ReLU    & 1
(4x5x5)                 & (2x5x5)                       & 1
150 Flatten             & 1
151 Concat
25 Relu
9 Relu
8 Relu
5


Supervised training on AIData0.txt lr0.001 epoch10 batch3 OptimizerAdam
Curated Reinforcement training level0 lr0.001 gamma0.8 food20 epsilon(0.9,0.1,1000) N4000 OptimizerAdagrad
This architecture seems to be onto something. After supervised training, there was clearly some sort of benefit, although the qualities were inaccurate. When this was turned over to CR learning, the results initially seem phenomenal See V3.png. The monkey is also consistently moving towards bananas.

Trying now this training method:
Supervised training on AIData0.txt lr0.001 epoch10 batch3 max_discount0.05 OptimizerAdam
CR training level0 lr0.001 gamma0.8 food20 epsilon(0.7,0.1,2000) N4000 OptimizerAdam
CR training level1 lr0.001 gamma0.8 food20 epsilon(0.3,0.1,2000) N4000 OptimizerAdam
CR training level0 lr0.001 gamma0.8 food20 epsilon(0.3,0.1,2000) N4000 OptimizerAdam
CR training level1 lr0.001 gamma0.8 food20 epsilon(0.3,0.1,2000) N4000 OptimizerAdam
CR training level2 lr0.001 gamma0.8 food20 epsilon(0.3,0.1,2000) N4000 OptimizerAdam
CR training level1 lr0.001 gamma0.8 food20 epsilon(0.3,0.1,2000) N4000 OptimizerAdam

Overall I am fairly happy with this. I will try now to set it free in the banana room (6% bananas)
T6: DQN lr0.01 gamma0.8 epsilon(0.3,0.1,5000) N50000 OptAdam
T7: CR level1 lr0.001 gamma0.8 food20 epsilon(0.15,0.15,1) N2000 OptAdam
T8: DQN lr0.001 gamma0.8 epsilon(0.3,0.1,5000) N50000 OptAdam

I notice that the monkey moves preferentially to the left. While this is garnering it a decent amount of food, this is not how a human would play, and it is not the most efficient method for moving around. My solution is to do curated training a further distance out so it will learn that it is beneficial to go up and down and not ignore up and down in favour of going left.
T9: CR level3 lr0.001 gamma0.8 food20 epsilon(0.15,0.15,1) N2000 OptAdam

Testing monkeys in random room (6% bananas)
 Model | Score
-------+-------
T9Model|  9.816, 9.828, 9.594, 9.990, 9.680
    AI | 39.407, 38.685, 39.991, 39.789, 39.555

Clearly the model is still insufficiently replicating the AI, however we are seeing an improvement over the linear AI. One of the issues we have is that the monkey is still missing things that are more than a few blocks away in the examples up to now. I think that this is primarily because outside of two blocks away, the monkey doesn't see exactly what is going on around it. This cropping occurs in the brain architecture. I think it is time to allow it to train in the banana room with some more complexity. I will go with a room that includes lava and barriers now. (2% barriers, 6% bananas, 1% lava). However, first I want to check it with curation of level 2 again briefly.


I had a great idea! I made it so that the initial move in curated training is always random. This way it explores in the first move (where it knows nothing) and then rely on its past curation training. I am starting a new training branch, called branch 1. Filenames for brain saves will be of the form B1Tnn.brainsave.

B1T0: CR lvl0 lr0.001 gamma0.8 food20 eps(0.3,0.1,2000) N10000 OptAdam
The monkey is not too accurate. I am not really sure why.
B1T1: CR lvl1 lr0.0005 gamma0.8 food20 eps(0.3,0.1,2000) N10000 OptAdam
I did some experimenting and it is a learning rate problem! The level 1 training does so much better with a smaller learning rate.

B1T0: CR lvl0 lr0.001 gamma0.8 food20 eps(0.3,0.1,2000) N10000 OptAdam rand_start
B1T1: CR lvl0 lr0.001 gamma0.8 food20 eps(0.3,0.1,2000) N3000 OptAdam
B1T2: CR lvl1 lr0.001 gamma0.8 food20 eps(0.3,0.1,2000) N5000 OptAdam

It's not doing too well honestly. I am going to let it do supervised training again. Keeping gamma at 0.8.

B2T0: Supervised AIData0.txt AIData1.txt AIData2.txt lr0.01 epoch10 batch3 max_discount0.05 OptAdagrad

From watching the monkey train, it is clear that the qualities are highly overestimated. This is to be expected because of the sheer number of bananas the AI monkey collects. The monkey seems to make the correct decision for movement some of the time.

B2T1: CR lvl0 lr0.001 gamma0.8 food20 eps(0.3,0.1,2000) N5000 OptAdam

The monkey is making all the right decisions at the 0th level of curation. Clearly pretraining with the AI is extremely important.

B2T2: CR lvl1 lr0.001 gamma0.8 food20 eps(0.3,0.1,2000) N5000 OptAdam

I noticed that before training the monkey was actually making decent decisions. I suppose that this must be due to B2T0 training. After training I don't see any marked improvement honestly. I notice that oftentimes the monkey doesn't know what to do when it gets adjacent to a banana as well. I will just train more and overwrite this last training.

B2T2: CR lvl1 lr0.001 gamma0.8 food20 eps(0.3,0.1,2000) N15000 OptAdam

It really didn't help.

B2T2: CR lvl1 lr0.001 gamma0.8 food20 eps(0.3,0.1,2000) N5000 OptAdam

Performance is quite good.

B2T3: CR lvl0 lr0.001 gamma0.8 food20 eps(0.3,0.1,2000) N5000 OptAdam

The model is a little too optimistic still about the rewards. I am going to let it run for a while in the random room to change that.

B2T4: RR lr0.001 gamma0.8 food20 eps(0.5,0.1,5000) N15000 OptAdam

Already before training I am testing the monkey on 2nd level curationn and it doesn't look bad. After training, the monkey retains its good curated training behavior

Monkey test

Testing monkeys in random room (2% barriers, 6% bananas, 1% lava)
 Model | Score
-------+-------
  B2T4 | 20.487, 21.365, 21.366, 21.068, 20.536
    AI | 31.863, 29.772, 29.655, 30.651, 30.145

This is indeed exciting! As we would expect, the AI does worse when we begin adding barriers (which must be traversed around) and lava. The B2T4 model also, on average, ends its turn with more food than it initally had! this means that, on average, it is self sustaining. Due to fluctuations of course it will still die most likely, but we are getting close to a fully self-sustaining monkey.

With this in mind I will do some level 0 training on lava. Followed by some level 2 training on bananas.

B2T5: CR lvl0 lr0.001 gamma0.8 food20 eps(0.3,0.1,5000) N5000 OptAdam LAVA

Now let's do some level 2 training with bananas.

B2T6: CR lvl2 lr0.001 gamma0.8 food20 eps(0.3,0.1,2000) N10000 OptAdam

The level 2 training didn't go well. The monkey lost its ability to make smart decision. We will revert back to B2T4.

B2T5: RR lr0.001 gamma0.8 food20 eps(0.5,0.05,5000) N50000 OptAdam

Let's test it again.

 Model | Score
-------+-------
  B2T4 | 31.411, 32.226, 31.097, 31.857, 31.087
    AI | 31.863, 29.772, 29.655, 30.651, 30.145

The monkey is about the same as the AI even a little better!!! I will make a gif of it playing (B2T5.gif).

What I would like to do at some point to make it play more like a human is to add some recurrency. However, right now I will just allow it to play for a while to get better. I am also going to reduce the banana frequency to 5%.

B2T6: RR lr0.001 gamma0.8 food20 eps(0.2,0.05,5000) N50000 OptAdam

 Model | Score
-------+-------
  B2T4 | 19.117, 19.650, 19.352, 19.283, 19.721
    AI | 26.500, 25.778, 24.676, 25.086, 23.639

The AI is outperforming again with sparser bananas. I will increase the barrier frequency to 3% and see what happens (#3%, m0%, b5%, d1%).

 Model | Score
-------+-------
  B2T4 | 18.901, 17.631, 19.069, 18.12, 17.987

B2T7: RR lr0.001 gamma0.8 food20 eps(0.1,0.05,5000) N50000 OptAdam

 Model | Score
-------+-------
  B2T4 | 26.505, 26.434, 35.357, 24.948, 24.583
    AI | 23.989, 21.699, 22.245, 20.497, 19.907

Watching this monkey go, I wonder if that the entire convolutional branch is pointless. The monkey doesn't seem to be using it at all. In fact, it completely ignores bananas that are far away. Anyway, the monkey is doing better than the AI now that we have added more barriers! That's a good place to stop today.

B2T8: CR lvl3 lr0.001 gamma0.8 food20 eps(0.3,0.1,2000) N10000 OptAdam

My goal is to have a more streamlined approach based on the previous approach.

B3T0: Supervised AIData0.txt-AIData4.txt lr0.01 epoch10 batch6 max_discount0.05 OptAdagrad

 Model | Score
-------+-------
  B3T0 | 4.171, 3.859, 4.156, 4.146, 3.966

B3T1: CR lvl0 lr0.001 gamma0.8 food20 eps(0.4,0.2,2000) N10000 OptAdam
B3T2: CR lvl1 lr0.001 gamma0.8 food20 eps(0.3,0.1,2000) N5000 OptAdam
B3T3: CR lvl2 lr0.001 gamma0.8 food20 eps(0.3,0.1,2000) N5000 OptAdam
B3T4: CR lvl0 lr0.001 gamma0.8 food20 eps(0.3,0.1,2000) N5000 OptAdam blocktype 'd'

Monkey test (#3%, m0%, b5%, d1%)

B3T5: RR lr0.001 gamma0.8 food20 eps(0.5,0.05,25000) N150000 OptAdam (#3%, m0%, b5%, d1)

Monkey test (#3%, m0%, b5%, d1%)

Nothing is working again. I really think it is because of the dirtiness of the data. I tried again with B2T0 and it works. I will try to replicate the results of B2T0.

B4T0: Supervised AIData0.txt AIData1.txt AIData2.txt lr0.01 epoch10 batch3 max_discount0.05 OptAdagrad
B4T1: CR lvl0 lr0.001 gamma0.8 food20 eps(0.3,0.1,2000) N5000 OptAdam
B4T2: CR lvl1 lr0.001 gamma0.8 food20 eps(0.3,0.1,2000) N5000 OptAdam

I haven't quite finished the above. Right now I am cleaning the data files of infinite loops. It seems that the vast majority of infinite loops are of length two (i.e. two states that feed into one another). Almost none are of length 1 and less than a percent seem to be of length 4. Length 3 is of course impossible.

Now I will repeat training B3 on the cleaned data to see what happened.

B5T0: Supervised AIData0CLEAN.txt lr0.01 epoch8 batch6 max_discount0.05 OptAdagrad

It doesn't work. Trying a new training method, overwriting B5. In the new method we just do supervised training with the cross entropy loss between the model's Q values and the action taken by the AI. It ended up working VERY well in getting the monkey to move in the right direction, but, as would be expected, the quality values don't actually mean anything. This will be recalibrated in some curated training.

B5T1: CR lvl0 lr0.001 gamma0.8 food20 eps(0.3,0.1,2000) N50000 OptAdam

It's looking good! However I do notice that it easily gets stuck in non-curated situations. This will have to be fixed in RR training.

B5T2: CR lvl1 lr0.001 gamma0.8 food20 eps(0.3,0.1,2000) N50000 OptAdam

That ruined it!

B5T2: CR lvl1 lr0.001 gamma0.8 food20 eps(0.3,0.1,2000) N5000 OptAdam

The monkey is performing slightly worse, but the qualities are more correct. I think this is good enough.

B5T3: CR lvl0 lr0.001 gamma0.8 food20 eps(0.3,0.1,2000) N2000 OptAdam block'd'

B5T4: RR lr0.001 gamma0.8 food20 eps(0.5,0.05,10000) N50000 OptAdam (#3%, m0%, b5%, d1)

Before training the monkey just always ends up standing still. Afterwards it moves down and finds nothing.

B5T0: Supervised AIData0CLEAN.txt-AIData5CLEAN.txt lr0.01 epoch8 batch6 max_discount0.05 OptAdagrad

Monkey test (#3%, m0%, b5%, d1%)
 Model | Score
-------+-------
  B5T0 | 30.307, 29.549, 30.068, 27.797, 29.180
    AI | 22.904, 22.500, 21.671, 19.885, 20.235

Already we see that the model does better than the AI. This is likely due to using cleaned data and the architecture of the neural net itself. Let's do some CR to fix the qualities.

B5T1: CR lvl0 lr0.001 gamma0.8 food20 eps(0.2,0.1,2000) N2000 OptAdam
B5T2: CR lvl1 lr0.001 gamma0.8 food20 eps(0.2,0.1,2000) N2000 OptAdam
B5T3: CR lvl2 lr0.001 gamma0.8 food20 eps(0.2,0.1,2000) N2000 OptAdam

The curated training may have ruined it! Let's try some RR to see if it is redeemable.

Monkey test (#3%, m0%, b5%, d1)
 Model | Score
-------+-------
  B5T2 | 12.441, 11.924, 12.278, 11.732, 12.176
  B5T3 | 4.361, 4.400, 4.287, 4.407, 4.311
  B5T4 | 2.709, 4.319, 4.203, 4.030, 4.147
  B6T1 | 20.476, 20.566, 20.636, 19.834, 20.660
  B6T0 | 29.783, 29.646, 28.895, 28.349, 28.757
  B6T2 | 26.02, 26.305, 24.994, 23.664, 24.105

B5T4: RR lr0.001 gamma0.8 food20 eps(0.2,0.05,10000) N50000 OptAdam (#3%, m0%, b5%, d1%)

It ended up being pretty stupid. I will work off of B5T0 again.

B6T0: B5T0
B6T1: RR lr0.001 gamma0.8 food20 eps(0.2,0.05,10000) N50000 OptAdam (#3%, m0%, b5%, d1%)

I believe the issue is that after B6T0 the performance is good because it trained against the AI but the qualities are all off. Then when we do reinforcement training, it throws those quality values more towards the correct values while destroying the accurate decision making structure. If we were to to SL followed by some RR, followed by SL again, it should work. SL will get the network in the right neighborhood for accurate decision making, RR will bring the qualities into range, and then SL should make the qualities similar to before. So if we do SL on B6T1, it will bring back the ddecision making, but the qualities will be more accurate. Then, we can do more RR to refine the AI.

B6T2: Supervised AIData0CLEAN.txt-AIData5CLEAN.txt lr0.01 epoch8 batch6 max_discount0.05 OptAdagrad

The qualities are way off again. I think I need to start with an AI that has qualities.

August 13, 2018
I finished writing the new AI that actually forms qualities. It is important to note that a more sophisticated AI could have been made, but the idea is that we can write something that is cobbled together and then have the monkey improve on it.


B7T0: Supervised AIQ.txt lr0.01 epoch3 batch6 OptAdagrad
B8T0: Supervised AIQ.txt lr0.005 epoch3 batch6 OptAdagrad
B8T0>B8T1: Supervised AIQ.txt lr0.001 epoch3 batch6 OptAdam
B9T0: Supervised AIQ.txt lr0.001 epoch3 batch6 OptAdam
B9T0: [(0, 1.159963676864431), (1, 0.3839519015815233), (2, 0.29398681060825244), (3, 0.246283497371789), (4, 0.2204386922023366), (5, 0.19181332473944612), (6, 0.166659355408144), (7, 0.15539608954418122), (8, 0.13654959302565997), (9, 0.12556600833205775), (10, 0.12001139864236249), (11, 0.11707362813501568), (12, 0.11153982498517423), (13, 0.10300092091143297), (14, 0.09810542092239706), (15, 0.09794530025311864), (16, 0.09712693241050831), (17, 0.0933433638854252)]
B9T0>B9T1: Supervised AIQ.txt lr0.0001 epoch3 batch6 OptAdam
B9T1: [(0, 0.08936239111418097), (1, 0.08508474309864933), (2, 0.08428258632290019), (3, 0.08348967887766519), (4, 0.08018727436117791), (5, 0.07764080053934375), (6, 0.07677416288165016), (7, 0.07698520274974957), (8, 0.0735351885543029), (9, 0.07116811417289577), (10, 0.07203058709159088), (11, 0.07043971272836361)]

Monkey test (#3%, m0%, b5%, d1)
 Model | Score
-------+-------
  B7T0 | 5.227, 5.088, 4.835, 4.966, 4.85
  B7T1 | 42.456, 43.44, 44.861, 43.191, 44.592 average 43.708
  B8T0 | 4.42
  B9T0 | 32.677, 32.653, 33.766, 32.004, 31.737
  B9T1 | 34.805, 34.914, 35.647, 36.014, 34.986


B9T2: RR lr0.001 gamma0.8 food20 eps(0.1,0.05,10000) N50000 OptAdam (#3%, m0%, b5%, d1%)

I realized I made an error in the AI where it wasn't correctly checking for barriers and danger around itself. I have to regenerate all the data.

B10T0: Supervised AIQ.txt lr0.001 epoch3 batch6 OptAdam
B10T0: [(0, 0.7118058174188239), (1, 0.5368936589102467), (2, 0.47634852371743325), (3, 0.4590919623883152), (4, 0.4242624473747127), (5, 0.41937614541992385), (6, 0.4316460456945558), (7, 0.414451541598901), (8, 0.40463973179179163), (9, 0.3979706317295995), (10, 0.35652733518625046), (11, 0.34628579537563636), (12, 0.34644300185361), (13, 0.3116296750875759), (14, 0.2917572970034352), (15, 0.2832087464096649), (16, 0.283203327573345), (17, 0.2652162148261154)]

B10T0>B10T1: Supervised AIQ.txt lr0.0001 epoch2 batch6 OptAdam
B10T1:[(0, 0.2496050657726996), (1, 0.23334774751405069), (2, 0.23704588304919352), (3, 0.21369923493337065), (4, 0.1954035487456129), (5, 0.18427802857059938), (6, 0.18273047322075123), (7, 0.18167273716043497), (8, 0.170218082672279), (9, 0.18139756227718185), (10, 0.17215481813844713), (11, 0.17249561768723576)]

B10T1>B10T2: RR lr0.001 gamma0.8 food20 eps(0.1,0.05,10000) N50000 OptAdam (#3%, m0%, b5%, d1%)

Monkey test (#3%, m0%, b5%, d1)
  Model | Score
--------+-------
  B10T0 | 31.895, 33.088, 33.407, 32.456, 31.387
  B10T1 | 33.198, 33.267, 34.292, 33.666, 32.863
  B10T2 | 27.995, 27.075, 26.281, 25.824, 26.330

I have rewritten the trainer to separate different lives of the monkey into different files. I have also changed the loss function to the L1 Clamp loss and am doing supervised learning with generated quality values again. We are also learning in *real* batches now.

B11T0: Supervised lr0.001 gamma0.8 food20 epoch100 batch 20 life0-life500 OptAdam (#3%, m0%, b5%, d1%) [(0, tensor(7.1623)), (1, tensor(7.1423)), (2, tensor(7.0602)), (3, tensor(6.9052)), (4, tensor(6.7504)), (5, tensor(6.6127)), (6, tensor(6.4904)), (7, tensor(6.3826)), (8, tensor(6.3078)), (9, tensor(6.2494)), (10, tensor(6.1948)), (11, tensor(6.1360)), (12, tensor(6.0717)), (13, tensor(6.0000)), (14, tensor(5.9165)), (15, tensor(5.8169)), (16, tensor(5.6921)), (17, tensor(5.5462)), (18, tensor(5.3848)), (19, tensor(5.2134)), (20, tensor(5.0408)), (21, tensor(4.8792)), (22, tensor(4.7341)), (23, tensor(4.6085)), (24, tensor(4.4857)), (25, tensor(4.3721)), (26, tensor(4.2670)), (27, tensor(4.1720)), (28, tensor(4.0766)), (29, tensor(3.9796)), (30, tensor(3.8857)), (31, tensor(3.7805)), (32, tensor(3.6760)), (33, tensor(3.5674)), (34, tensor(3.4761)), (35, tensor(3.3747)), (36, tensor(3.2902)), (37, tensor(3.2192)), (38, tensor(3.1481)), (39, tensor(3.0864)), (40, tensor(3.0218)), (41, tensor(2.9620)), (42, tensor(2.8925)), (43, tensor(2.8362)), (44, tensor(2.7811)), (45, tensor(2.7318)), (46, tensor(2.6746)), (47, tensor(2.6494)), (48, tensor(2.5916)), (49, tensor(2.5658)), (50, tensor(2.5262)), (51, tensor(2.4887)), (52, tensor(2.4618)), (53, tensor(2.4462)), (54, tensor(2.4208)), (55, tensor(2.3813)), (56, tensor(2.3567)), (57, tensor(2.3687)), (58, tensor(2.3290)), (59, tensor(2.2982)), (60, tensor(2.2862)), (61, tensor(2.2695)), (62, tensor(2.2485)), (63, tensor(2.2278)), (64, tensor(2.2173)), (65, tensor(2.2115)), (66, tensor(2.1898)), (67, tensor(2.1743)), (68, tensor(2.1661)), (69, tensor(2.1520)), (70, tensor(2.1424)), (71, tensor(2.1230)), (72, tensor(2.1139)), (73, tensor(2.1091)), (74, tensor(2.0863)), (75, tensor(2.0749)), (76, tensor(2.0748)), (77, tensor(2.0602)), (78, tensor(2.0458)), (79, tensor(2.0402)), (80, tensor(2.0331)), (81, tensor(2.0191)), (82, tensor(2.0177)), (83, tensor(2.0042)), (84, tensor(2.0012)), (85, tensor(1.9917)), (86, tensor(1.9815)), (87, tensor(1.9771)), (88, tensor(1.9718)), (89, tensor(1.9602)), (90, tensor(1.9465)), (91, tensor(1.9392)), (92, tensor(1.9322)), (93, tensor(1.9322)), (94, tensor(1.9319)), (95, tensor(1.9278)), (96, tensor(1.9091)), (97, tensor(1.9065)), (98, tensor(1.9022)), (99, tensor(1.9008))]

B11T0>B11T1: Supervised lr0.001 gamma0.8 food20 epoch100 batch 20 life0-life500 OptAdam
B11T1>B11T2: Supervised lr0.0005 gamma0.8 food20 epoch100 batch 20 life0-life500 OptAdam

Monkey test (#3%, m0%, b5%, d1)
  Model | Score
--------+-------
  B11T0 | 4.201
  B11T1 | 4.559
  B11T2 | 2.812


So I have stopped recording everything but I suppose I should write my findings. Firstly, I impolemented ReduceLROnPlateau and am using Adagrad again.

I have discovered that BrainV4 isn't really a suitable architecture.  BrainV4 trains to 0.67 on the training set (life1-life19) but gives a loss of 0.95 on the validation set (life0) whereas brain V5 trains to 0.79 on the trainin set and 0.84 on the validation set. Clearly the addition of the convolution is preventing overfitting and allowing for a moregeneralized fit to the underlying pattern of the data. This was not with the L1_clamp_loss but the L1_no_clamp loss. With L1_clamp_loss these numbers go to 0.98, 1.63, 1.07 and 1.29 respectively. This leads to the same conclusion.

I increased the training set to (life1-life99) and the loss and validation loss are staying closer together (1.18, 1.26). This is good, as it means that the generality is increasing. However, this is still not good enough. I will have to devise a new architecture.

Going back to (life1-life19) for brevity. Brain V6 gives (0.91, 1.12). It scores 12!!!!! All I did was expand the convolutional branch for brain 6. I will make it bigger for version 7

Convolutions are specified as (kernel size, padding, stride)

vision                  & vision                         & food
(4x11x11)               & (4x11x11)                      & 1
(4x5x5) Crop            & (12x11x11) Conv2d(3,1,1) ReLU  & 1
(4x5x5) Crop            & (12x11x11) Conv2d(3,1,1) ReLU  & 1
(4x5x5)                 & (6x8x8) Conv2d(4,0,1) ReLU     & 1
(4x5x5)                 & (2x5x5) Conv2d(4,0,1) ReLU     & 1
(4x5x5)                 & (2x5x5)                        & 1
150 Flatten             & 1
151 Concat
25 Relu
9 Relu
8 Relu
5

V7 does slightly better in terms of generality (0.94, 1.06). Fewer epochs were done. OMG IT SCORED A 21!!! It is self sustaining! I am having trouble reproducing this. This lack of reproducability may be a reason to look at some sort of simulated annealing. I couldn't reproduce the V7 result but I could reproduce the V6 result (0.98, 1.16) and scored 14.

I have been playing around. It turns out the batch size is VERY important. I didn't know this.

I moved onto life1-life199 and did 100 epochs of 130 batches (256 data points each batch) and got losses (0.82,0.89). I tested the model and scored (25.294, 25.320, 23.63, 24.804, 23.655). Loss seemed to cap out. To recall, the AI performs in the range of a score of 43.

I want to try a really deep fully connected network because I still have a hunch that convolutions are too coarse grain for a problem like this. When I solve this problem, I don't even think about that. If I were to guess how the net works, I would say that in the first hidden layer, every node represents a path that the monkey could take and its value would be the quality of that move. Let's say it only considers paths with 6 moves. Ther number of paths is 15625 but the vast majority of these paths are plain dumb. Perhaps there are only a few hundred "intelligent" paths. Then the first hidden layer has maybe 400 nodes. Now, 100 of these say the first move is 'w', 100 for 'a', and so on. Theyse 100 nodes need to decide which among them has the highest quality. It could get the maximum quite well with a couple layers of size 20 and 10 respectively (I guess). So that gives a second hidden layer of size 80 and a third hidden layer of size 40. Then we just have 5 possible moves these feed into. I think I can do this in brain V4.

Wow! I didn't expect this! It is overfitting like crazy (0.34, 0.83) scoring (37.935, 38.048, 37.867, 38.729, 38.981). Not to mention it runs MUCH faster than the AI. I am going to try paring down the size of the maximum finders by a factor of 5. Scored 27 after that. I want to increase its expressive power by giving it access to more path possibilities. 1000 possible paths. It didn't really help (0.23, 0.83) score (35.19). 300 possible paths. (0.47, 0.83) (35.97). 200 paths. (0.48, 80) (38.56) 150 paths (0.53, 0.88) (37.81). 80 paths (0.66, 0.84) (34.92).

I will stick with 200 in the first layer. The overfitting is indicative of room to change later on for optimization. I will try again with all the data: 100 epochs of 325 batches of 244 data points. It is getting a better validation score now! (0.57, 0.75) (37.818, 38.887, 39.011, 39.760, 38.483). Save is in brainsave\V4_480-200-16-8-4_T0.brainsave. Now doing reinforcement learning. It made it worse (31.803), but that is kind of expected. I will try training for longer.

Actually, what I should do is investigate how the monkey behaves with/without the prior.

Training with (100 000WP, 200 000WoP) trials with lr or 0.001 using the Adam optimizer. 

WP (With Prior): brainsave\V4_480-200-16-8-4_T0.brainsave --> brainsave\V4_480-200-16-8-4_T1_WP.brainsave
WoP (Without Prior): brainsave\V4_480-200-16-8-4_T0.brainsave --> brainsave\V4_480-200-16-8-4_T1_WoP.brainsave

epsilon_WP: (0.05, 0.01, 10000) score (28.642, 27.893, 27.726, 28.49, 28.33) 
epsilon_WoP: (0.7, 0.01, 10000) score (4.474, 4.253, 4.263, 3.936, 4.287)

epsilon_WP: [(0, 39.18), (1000, 23.2), (2000, 20.14), (3000, 18.02), (4000, 15.36), (5000, 20.3), (6000, 25.14), (7000, 19.5), (8000, 25.06), (9000, 20.76), (10000, 17.6), (11000, 22.48), (12000, 21.06), (13000, 30.8), (14000, 17.24), (15000, 29.02), (16000, 22.76), (17000, 21.28), (18000, 18.84), (19000, 20.14), (20000, 23.82), (21000, 30.16), (22000, 33.02), (23000, 16.78), (24000, 19.5), (25000, 26.28), (26000, 17.72), (27000, 23.32), (28000, 18.5), (29000, 26.38), (30000, 24.02), (31000, 26.12), (32000, 29.4), (33000, 18.7), (34000, 32.2), (35000, 29.54), (36000, 29.94), (37000, 29.38), (38000, 36.06), (39000, 15.14), (40000, 29.38), (41000, 33.3), (42000, 32.18), (43000, 27.24), (44000, 25.3), (45000, 27.3), (46000, 35.02), (47000, 28.82), (48000, 27.1), (49000, 28.1), (50000, 32.78), (51000, 31.84), (52000, 35.5), (53000, 33.92), (54000, 27.68), (55000, 33.82), (56000, 33.84), (57000, 32.66), (58000, 33.34), (59000, 37.76), (60000, 33.3), (61000, 34.16), (62000, 34.1), (63000, 35.86), (64000, 22.9), (65000, 29.38), (66000, 27.32), (67000, 32.36), (68000, 30.94), (69000, 34.24), (70000, 22.02), (71000, 24.8), (72000, 30.46), (73000, 37.18), (74000, 24.28), (75000, 36.52), (76000, 32.06), (77000, 30.66), (78000, 32.62), (79000, 30.44), (80000, 33.28), (81000, 36.48), (82000, 32.5), (83000, 27.76), (84000, 28.88), (85000, 25.74), (86000, 36.36), (87000, 32.0), (88000, 33.46), (89000, 35.02), (90000, 37.64), (91000, 22.82), (92000, 40.5), (93000, 31.56), (94000, 30.76), (95000, 22.24), (96000, 32.82), (97000, 36.04), (98000, 23.14), (99000, 34.14)]


There you have it! The proof is in the pudding, see img\V4_480-200-16-8-4_T1_WP.png. The score decreases at the start as you may expect, but is on the way to recovery. This should be done again for maybe 10 times as long.

Continued training
1 000 000 points with epsilon (0.05, 0.02, 100 000)
T2: brainsave\V4_480-200-16-8-4_T0.brainsave --> brainsave\V4_480-200-16-8-4_T2_WP.brainsave
T2: score (37.032, 36.644, 37.011, 37.751., 37.581) average 37.201
T2: [(0, 24.72), (10000, 35.88), (20000, 35.2), (30000, 36.76), (40000, 44.68), (50000, 36.28), (60000, 31.88), (70000, 32.5), (80000, 37.76), (90000, 36.1), (100000, 39.78), (110000, 21.6), (120000, 30.2), (130000, 32.5), (140000, 33.64), (150000, 42.36), (160000, 33.9), (170000, 32.28), (180000, 28.6), (190000, 28.18), (200000, 34.2), (210000, 35.2), (220000, 37.68), (230000, 36.42), (240000, 32.64), (250000, 35.22), (260000, 33.24), (270000, 41.88), (280000, 37.82), (290000, 33.6), (300000, 33.96), (310000, 34.5), (320000, 31.54), (330000, 39.18), (340000, 38.94), (350000, 36.54), (360000, 33.62), (370000, 38.28), (380000, 34.1), (390000, 36.78), (400000, 36.2), (410000, 39.84), (420000, 35.46), (430000, 32.54), (440000, 33.52), (450000, 34.34), (460000, 36.56), (470000, 36.42), (480000, 38.74), (490000, 42.6), (500000, 44.28), (510000, 32.26), (520000, 39.68), (530000, 36.02), (540000, 36.28), (550000, 35.04), (560000, 35.34), (570000, 36.22), (580000, 37.34), (590000, 38.16), (600000, 39.96), (610000, 36.0), (620000, 36.18), (630000, 40.26), (640000, 31.62), (650000, 36.34), (660000, 41.56), (670000, 38.26), (680000, 35.36), (690000, 31.04), (700000, 37.66), (710000, 41.88), (720000, 36.92), (730000, 32.04), (740000, 40.76), (750000, 37.98), (760000, 42.78), (770000, 35.54), (780000, 30.4), (790000, 37.1), (800000, 40.16), (810000, 39.76), (820000, 37.64), (830000, 30.94), (840000, 40.1), (850000, 34.42), (860000, 41.4), (870000, 32.72), (880000, 35.44), (890000, 32.0), (900000, 41.2), (910000, 40.78), (920000, 40.9), (930000, 37.12), (940000, 34.28), (950000, 40.28), (960000, 35.18), (970000, 35.96), (980000, 34.9), (990000, 36.66)]

So we see it is not training past the efficiency of the AI, but it has developed a stable Q function that performs at about 85% of the score of the AI. The question now is whether we can find a situation (like a real game room) where we can train the monkey to play better than the AI.

Making a new map based on the Cogmind procedurally generated map https://www.gridsagegames.com/blog/gsg-content/uploads/2014/06/cogmind_map_tunnelers_even.png

I have the map reader working. So now I should test the AI and the NN on this map.
T2 score (18.224, 16.990,, 16.967, 17.394, 16.272)
AI score (31.335, 30.405, 30.910 29.125, 29.51)

So the AI is scoring much better than the NN. Maybe I should generate more training data on a very large map to train on. Anyway, for now I will try just training the NN on the map.

T3: Same as T2 except for ...T2_WP.brainsave --> ...T3_WP.brainsave score 7.896, 7.015, 7.805, 6.965, 6.943.

I accidentally overwrote all the old data files when I was making new files so I regenerated them. Going to train a brain on it, overwriting T3.

T3: life_adventure[0,500).txt with 100 epochs of 325 batches of 372 data points each.
The fit honestly isn't that good, but if you look at the score, it is quite impressive (34.656, 34.803, 34.464, 34.734, 34.88). Clearly the prior is sufficiently trained.
[(0, 5.824313976214483), (1, 5.46854816363408), (2, 5.285904935323275), (3, 5.128177200464102), (4, 4.980209785608145), (5, 4.830693100415743), (6, 4.703790353995103), (7, 4.607782823122465), (8, 4.534893312454224), (9, 4.47099077958327), (10, 4.418908722584064), (11, 4.370608290892381), (12, 4.333685227174025), (13, 4.31362451003148), (14, 4.294269711054288), (15, 4.275046881895799), (16, 4.257353048691383), (17, 4.238512747104352), (18, 4.2232530432481035), (19, 4.207198229936453), (20, 4.191515500362103), (21, 4.174556675324073), (22, 4.158652979043814), (23, 4.145779307438777), (24, 4.137495840879587), (25, 4.130875236437871), (26, 4.124064450997572), (27, 4.118056528384869), (28, 4.111689346386836), (29, 4.105635922138507), (30, 4.099804903177114), (31, 4.093433880072373), (32, 4.088540371014521), (33, 4.082557059801542), (34, 4.077469976865328), (35, 4.0738297656866225), (36, 4.072153787246117), (37, 4.069205852655264), (38, 4.066885815033546), (39, 4.064415622857901), (40, 4.061383402897762), (41, 4.059508419403663), (42, 4.057111120957595), (43, 4.054585501597478), (44, 4.052822754199688), (45, 4.050102632595943), (46, 4.049085751680227), (47, 4.047911704503573), (48, 4.047284111609826), (49, 4.046764750480651), (50, 4.044926507656391), (51, 4.044341426629287), (52, 4.043284008319561), (53, 4.042423190336961), (54, 4.04002181823437), (55, 4.039965419769287), (56, 4.039146421872653), (57, 4.037674387051509), (58, 4.037791979496296), (59, 4.037085386422964), (60, 4.037686866613535), (61, 4.037061264698322), (62, 4.03652413551624), (63, 4.0357431096297045), (64, 4.035066345655001), (65, 4.034216289153465), (66, 4.033904028672438), (67, 4.033753687418424), (68, 4.033241899196918), (69, 4.034202889295725), (70, 4.033486994596628), (71, 4.032959247002235), (72, 4.031904340890738), (73, 4.032561429097102), (74, 4.032627133956322), (75, 4.032387784811166), (76, 4.032250279279856), (77, 4.032661530788128), (78, 4.032456284669729), (79, 4.0320160953815165), (80, 4.031813220977783), (81, 4.031511567922739), (82, 4.030788634373591), (83, 4.031335781537569), (84, 4.031244439711937), (85, 4.031463577197148), (86, 4.0316007159306455), (87, 4.0312907174917365), (88, 4.030869811131404), (89, 4.030566922334525), (90, 4.030650410652161), (91, 4.030801291098961), (92, 4.0306815609565145), (93, 4.031062625371493), (94, 4.029931408075186), (95, 4.030223730894235), (96, 4.03036234158736), (97, 4.030903675372784), (98, 4.031031969143794), (99, 4.031013612013597)]

Most interestingly, because this was done on a pre-existing prior, the training was very fast, only taking about 30 epochs to finish.
I will let it to RR to see if it can improve. I need to keep the learning rate low enough that it doesn't blow the prior out of the water but slowly relaxes into a better Q function.

I am going in a new direction. I will use brainsave\V4_480-200-16-8-4_T0.brainsave, but then train on the adventure map to get brainsave\\V4_480-200-16-8-4_T1.brainsave working close to the same efficiency as the AI and then do RR. score (24.75, 22.99, 22.841, 22.291, 22.713) lr 0.01 optimizer Adagrad, 650 batches of 186 data points, 40 epochs.

Now doing neural net training.

I have changed to a reward of -10 for death. Having a such a big penalty was causing too many issues with the fit.

Just finished the initial training on the adventure room. 100 epochs of 300 batches of 132 points. Overfitting after 20 epochs. It is scoring a 20 though, not great. I am going to increase the dataset size. I think I am going to have to implement a grid search test to find hyperparameters. Learning rate is too high, batch number isn't clear, etc.

So I was having a lot of trouble, then I finally decided to take a look at the training data and it was crazy. I don't know what happened. Some of it was the random room, some of it was the adventure map. I really can't say what happened. Regerating only 50000 points now.

With the AI guided training, the monkey performs alright, degrading slowly, but ultimately it still degrades. I believe this is becasue the monkey was never actually trained on death and every death just kicks the NN in the nuts. We need to deal with the training data in a more realistic way (including death). Right now I am thinking maybe we should just truncate the series with a -1.31 for an average value.

This helped! Training for death helped and you see after RR the monkey actually has begun to understand death. In the dataset there are less than 600 deaths in 26000 points (2%), and in RR death occurs at approximately this rate, so I am happy enough with it. What I shall do now is investigate scenarios where the loss is greater than 12. I don't understand why this is happening.

Made a new map from https://www.gridsagegames.com/blog/2014/06/mapgen-cellular-automata/ https://www.gridsagegames.com/blog/gsg-content/uploads/2014/06/cogmind_map_composite.png

New training where the guide is removed and the monkey explores

    monkey_brain.load_state_dict(torch.load('brainsave\\batch269lr0.00036869445466436446.brainsave'))

    # Reinforcment learning
    epsilon_guide = train.epsilon_interpolation([0,25,75,100],[0,0.5,1,1])
    epsilon_explore = train.epsilon_interpolation([0,40,80,100],[0,0,0.02,0])
    loss_report = train.guided_dqn(g, test_g, 100000, 0.8, 0.01, \
    AI_brain, epsilon_guide, epsilon_explore)

    torch.save(monkey_brain.state_dict(), 'brainsave\\total_training.brainsave')

See NotLongEnough.png. It is clear that we were premature in allowing the monkey to control itself. The monkey needs to train for a longer period of time with the guide, and transition to not being guided slower. We will try with this,

    monkey_brain.load_state_dict(torch.load('brainsave\\batch269lr0.00036869445466436446.brainsave'))

    # Reinforcment learning
    epsilon_guide = train.epsilon_interpolation([0,10,20,40,80,100],[0,0,0.2,0.3,0.8,1])
    epsilon_explore = train.epsilon_interpolation([0,40,80,100],[0,0,0.02,0])
    loss_report = train.guided_dqn(g, test_g, 100000, 0.8, 0.01, \
    AI_brain, epsilon_guide, epsilon_explore)

    torch.save(monkey_brain.state_dict(), 'brainsave\\total_training.brainsave')

See ReducingLoss.png. Clearly the loss is decreasing which is nice, but we are still making the monkey worse.

Those last tests didn't work quite right because of an error with train.epsilon_interpolation. Will redo.

See Phase2.png.

Tests 31.265, 31.475, 30.574, 30.401, 28.889.
AI tests 

I kind of want to switch to bananas having 10 food and having less bananas around. I think I am going to do it.

New data in data10 folder.

===============================================================================================================================
====================================================== THE NEW ERA ============================================================
===============================================================================================================================

Tests are now constituted of 300 trials of 50 turns. Mean(std) will be reported.

Guide tests 69(37) on AdventureMapBananaLavaShrunkMediumBanana.png

Guide tests 42(29) on AdventureMapBananaLavaShrunk.png

Generating new data on AdventureMapBananaLavaShrunk.png in data\AdventureMapBananaLavaShrunk with 50 000 turns and a turn maximum of 100.

Did a grid search with brain V8
train.grid_search_supervised(brain.BrainV8, 30, (250, 320, 10), (-3.5,-2.5,10), paths, gamma, room_start, 'grid_search\\')

The best that the model does is 23(24). I think we need to investigate new architectures that bring us closer to the range that the guide does.

Brain V8
        self.f1 = nn.Linear(485,8)
        self.f2 = nn.Linear(8,5)

Brain V9 has structure
        self.f1 = nn.Linear(485,8)
        self.f2 = nn.Linear(8,5)
        self.f3 = nn.Linear(5,5)

Adding another layer did not improve the model whatsoever. It seems to actually make it worse. I am going to try V10 where the hidden layer is made smaller than V8.

BrainV10
        self.f1 = nn.Linear(485,6)
        self.f2 = nn.Linear(6,5)

I think it's doing better. I am going to try a different brain with a tiny hidden layer.

BrainV11
        self.f1 = nn.Linear(485,3)
        self.f2 = nn.Linear(3,5)

It finally got worse. I am going to try making it a little bigger.

BrainV12
        self.f1 = nn.Linear(485,5)
        self.f2 = nn.Linear(5,5)

This is worse than V10 as well. Clearly 6 nodes in the hidden layer is the minimum.
Using type BrainV10 'brainsave\\batch226lr0.0029763521160930395.brainsave' with score 22(23)

I have finally finished developing the guide search which does a grid search for the hyperparameters of the second phase (epsilon_guide and epsilon_explore). We will finally determine if it is tunable or not in training. Perhaps, as well, we need to switch from AdventureMapBananaLavaShrunk.png to a map with more bananas. It is hard to say. My gut tells me that V10, although it performs equally as well as V8 in Phase I, may not have enough expressive power for adapting in Phase II and Phase III. I saved all of it in guide search V10. It is not good enough. It also seems that the training didn't go for long enough as evidenced by the sharp increase og the guide epsilon in the end.

What I will do now is switch back to the V8 one.

I accidentally overwrote some of the files in the latest grid search (V10). Oh well.

Doing a guide search on brain V8 batch306lr0.0006812919164076447.brainsave

guide_range = [0,0.1,0.2,0.35,0.5, 0.65,0.8,0.9,1]
explore_range = [0,0.01,0.03,0.06]
monkey_brain.load_state_dict(torch.load('brainsave\\batch306lr0.0006812919164076447.brainsave'))
total_training_data, percentage_list, epsilon_guide_history, epsilon_explore_history = \
    train.guide_search(g, test_g, gamma, lr_reinforcement, AI_brain, \
    guide_range, explore_range, \
    12, 10000, 'guide_searh\\guide_search_V8', initial_phase=20000, testing_tuple=(300,50))

The monkey didn't really do that much better in terms of score, but in terms of loss we are seeing some improvement. We should try an overfit one.

Brain V13
        self.f1 = nn.Linear(485,8)
        self.f2 = nn.Linear(8,5)

Running a grid search on 'maps\\AdventureMapBananaLavaShrunk.png'
    DEATH_REWARD = -10
    BANANA_FOOD = 5
    paths = ['data\\AdventureMapBananaLavaShrunk\\life'+str(i)+'.dat' for i in range(750)].
    train.grid_search_supervised(brain.BrainV8, 30, (100, 520, 20), (-4,-2,20), paths, \
    gamma, room_start, 'grid_search\\V13\\')

Doing a guide search.
    guide_range = [0,0.1,0.2,0.35,0.5, 0.65,0.8,0.9,1]
    explore_range = [0,0.01,0.03,0.06]
    monkey_brain.load_state_dict(torch.load('brainsave\\batch100lr0.009999999776482582.brainsave'))
    total_training_data, percentage_list, epsilon_guide_history, epsilon_explore_history = \
        train.guide_search(g, test_g, gamma, lr_reinforcement, AI_brain, \
        guide_range, explore_range, \
        12, 10000, 'guide_search\\guide_search_V13', initial_phase=20000, testing_tuple=(300,50))

It actually is doing better! It is scoring higher and has a lower loss. I think that we are on to something! Additionally, the exploration seems to be less up and down. To reduce computational strain, I am going to override the search for the explore level. Additionally I am going to make the brain more complex yet again. I may even end up adding another layer some day.

Brain V14
        self.f1 = nn.Linear(485,16)
        self.f2 = nn.Linear(16,5)

Running a grid search on 'maps\\AdventureMapBananaLavaShrunk.png'
    DEATH_REWARD = -10
    BANANA_FOOD = 5
    paths = ['data\\AdventureMapBananaLavaShrunk\\life'+str(i)+'.dat' for i in range(750)].
    train.grid_search_supervised(brain.BrainV14, 30, (100, 520, 20), (-4,-2,20), paths, \
    gamma, room_start, 'grid_search\\V14\\')

Running a guide search on that.
    guide_range = [0,0.1,0.2,0.35,0.5, 0.65,0.8,0.9,1]
    explore_range = [0,0.01,0.03,0.06]
    explore_override = [0,0,0.01,0.02,0.03,0.04,0.05,0.04,0.05,0.02,0.01,0]
    monkey_brain.load_state_dict(torch.load('brainsave\\batch100lr0.0004281332076061517.brainsave'))
    total_training_data, percentage_list, epsilon_guide_history, epsilon_explore_history = \
        train.guide_search(g, test_g, gamma, lr_reinforcement, AI_brain, \
        guide_range, explore_range, \
        12, 10000, 'guide_search\\guide_search_V14', initial_phase=20000, testing_tuple=(300,50), \
        override_explore = explore_override)

It seems almost like it was cut short. Encouragingly the loss is still getting better.

Well, it's not good enough until we overdo it. Let's make a more complicated brain.

Brain V15
        self.f1 = nn.Linear(485,25)
        self.f2 = nn.Linear(25,5)

Running a grid search on 'maps\\AdventureMapBananaLavaShrunk.png'
    DEATH_REWARD = -10
    BANANA_FOOD = 5
    paths = ['data\\AdventureMapBananaLavaShrunk\\life'+str(i)+'.dat' for i in range(750)].
    train.grid_search_supervised(brain.BrainV15, 30, (100, 520, 20), (-4,-2,20), paths, \
    gamma, room_start, 'grid_search\\V15\\')

Running a guide search.
    guide_range = [0,0.1,0.2,0.35,0.5, 0.65,0.8,0.9,1]
    explore_range = [0,0.01,0.03,0.06]
    explore_override = [0,0,0.01,0.02,0.03,0.04,0.04,0.04,0.03,0.02,0.01,0]
    monkey_brain.load_state_dict(torch.load('brainsave\\batch499lr0.0004281332076061517.brainsave'))
    total_training_data, percentage_list, epsilon_guide_history, epsilon_explore_history = \
        train.guide_search(g, test_g, gamma, lr_reinforcement, AI_brain, \
        guide_range, explore_range, \
        12, 10000, 'guide_search\\guide_search_V15', initial_phase=20000, testing_tuple=(300,50), \
        override_explore = explore_override)

Well it didn't do as well score wise or loss wise as other brains, but what I find most interesting about this brain is that its loss value is decreasing steadily as opposed to other brains where it is a little sporadic. Additionally, it didn't seem to be done training. If I just run the Phase 2 training for a longer time I think it will not be so good because it will take a very long time. However if you take a look at the code, allowing exploration only really doubles the amount of computation which isn't that big of a deal I think. I am going to try bumping the V15 test up to Phase 3 for a couple days to see what happens.

So I ran a short Phase 3 (50000 points) just to see what happened. It was encouraging that the score did not decrease, so the neural net has become stable.

I ran it for 500 000 points and we start to see a definite improvement! I am goint to run it for 5 000 000 points!.

train.unguided_dqn(g, test_g, 5000000, gamma, lr_reinforcement, \
            testing_tuple = (300, 50), epsilon_explore = lambda x: 0.03)

So it seems the performance of the net plateaued right around the end of the first test (with 50 000 points). Say 60 00 is where it plateaued with a score in the high 20s. I will try doingit again with less exploration and more exploration (50000 points).

So I did the one with less exploration,
        phase_3_data = train.unguided_dqn(g, test_g, 5000000, gamma, lr_reinforcement, \
            testing_tuple = (300, 50), epsilon_explore = lambda x: 0.01)
I was a little surprised to see that it did a little better! It increased slower but it ended up higher. I should try a decaying value. Maybe something starting at 0.03 and decreasing to 0 by the end of things. What concerns me a bit is that the loss remains high, so the monkey really isn't understanding death.


    if phase == 3:
        filename = 'Phase_3_V15'
        monkey_brain.load_state_dict(torch.load('brainsave\\Phase_2_V15.brainsave'))
        phase_3_data = train.unguided_dqn(g, test_g, 5000000, gamma, lr_reinforcement, \
            testing_tuple = (100, 50), epsilon_explore = lambda x: 0.3 * np.exp(-4 * x))
        torch.save(monkey_brain.state_dict(), 'brainsave\\%s.brainsave' % (filename,))
        plot_phase_3(phase_3_data, save_path = 'img\\%s.png' % (filename,))
        with open('Phase_3_Data\\%s.txt' % (filename,), 'w') as out_f:
            out_f.write(str(phase_3_data))
        exit()

I have started keeping track of averages over a greater number of trials. It gives a more realistic plot of loss. I should also probably move to the medium banana map.

############## A BIG CHANGE ################
I have started showing some averaged values of loss and score (averaged over some number of iterations.) I think it makes sense to restart training starting from phase 2 (I am happy with phase 1).

I will try again with both V8 and V15

        folder = 'Phase_2_Data\\V15'
        guide_range = [0,0.1,0.2,0.35,0.5, 0.65,0.8,0.9,1]
        explore_range = [0,0.01,0.03,0.06]
        explore_override = [0.00, 0.01, 0.01, 0.02, 0.02, 0.03, 0.03, 0.03, 0.04, 0.04, 0.04, 0.04, 0.03, 0.03, 0.03, 0.02, 0.02, 0.01, 0.01, 0.00]
        monkey_brain.load_state_dict(torch.load('brainsave\\batch499lr0.0004281332076061517.brainsave'))
        total_training_data, percentage_list, epsilon_guide_history, epsilon_explore_history = \
            train.guide_search(g, test_g, gamma, lr_reinforcement, AI_brain, \
            guide_range, explore_range, \
            10, 5000, folder, initial_phase=10000, testing_tuple=(50,50), \
            override_explore = explore_override, number_of_tests = 20)

        folder = 'Phase_2_Data\\V8'
        guide_range = [0,0.1,0.2,0.35,0.5, 0.65,0.8,0.9,1]
        explore_range = [0,0.01,0.03,0.06]
        explore_override = [0,0,0.01,0.02,0.03,0.04,0.04,0.04,0.03,0.02,0.01,0]
        monkey_brain.load_state_dict(torch.load('brainsave\\batch264lr0.003162277629598975.brainsave'))
        total_training_data, percentage_list, epsilon_guide_history, epsilon_explore_history = \
            train.guide_search(g, test_g, gamma, lr_reinforcement, AI_brain, \
            guide_range, explore_range, \
            10, 5000, folder, initial_phase=10000, testing_tuple=(50,50), \
            override_explore = explore_override, number_of_tests = 20)

Interestingly, both brains started with a similar level of performance, but with BrainV15 (the one with more hidden nodes) the neural net ends up getting better whereas with V8 it gets worse as the neural net makes more of its own decisions. Clearly it takes more complexity in brain architecture to learn from reinforcement learning. I daresay that we may even benefit from having more hidden neurons.

So I tried re-running phase 2 on V15 and I see it relax down exactly to where it started, just like V8. Maybe more hidden nodes isn't better.

Phase 3 definitely improves it on a reduced size for V15. I need to try on a large training size for both.

The phase 3 data pretty conclusively shows that V15 is better. I will try by increasing training time by a factor of 10.

It seems that one million points is clearly enough. I may also change the exploration function from 0.03 * np.exp(-4 * x) to 0.04 * np.exp(-3 * x)

I am introducing a new brain.

Brain V16
        self.f1 = nn.Linear(485,50)
        self.f2 = nn.Linear(50,5)

I ran it through phases 1, 2, and 3 and it does quite well. It did better in phase 2 than the others but about the same in phase 3 as V15. I need to run phase 3 longer for all of them. The averaging makes that clear now. I will go 10 times longer.

I ran one 10 times longer and it seems 3 times longer is perfectly sufficient. I think the main issue currently is in phase 1. The net should be doing as well as the guide after phase 1. Definitely not a factor of 2 worse. I need to rethink it. The slope of the loss shouldn't matter because the learning rate is also a factor in that. Perhaps the issue is that the unchosen options aren incentivised to be lower than the chosen option. We may have better luck if we make the lower side of the clamp have a slight slope away from the maximum value.

I have finished writing that function. I will have to run a brain on it. Perhaps V15 first to see how it does.

HOLD ON A SECOND! I just remembered, the scores for phase 1 were all calculated on the old (50, 30) training tuple, so they are incorrect! I need to run them all again with the L1 clamp loss (non leaking).

class BrainV17(BrainV9):
        self.f1 = nn.Linear(485,50)
        self.f2 = nn.Linear(50,8)
        self.f3 = nn.Linear(8,5)


This is going in /Phase_1_Data/

Interestingly, V16 and V17 are about evenly tied for being the best (score 32). I think I will try running phase 2 on both of them.
V17_batch130lr0.005994840990751982.brainsave
V16_batch190lr0.002154435496777296.brainsave

It's great! V17 did a much better job in Phase 2. Now onto phase 3.

In phase 3 both V16 and V17 do a comparable job. The question is, will V17 continue to improve, or plateau just like V16.

There was a big error in V17. V17 called V4.__init__ instead of V9.__init__, making its real architecture,
        self.f1 = nn.Linear(485,50)
        self.f2 = nn.Linear(50,8)
        self.f3 = nn.Linear(8,5)
        self.f4 = nn.Linear(40,5)

I don't know what the hell that means. Anyway, I will make V18 which is what V17 was SUPPOSED TO BE.
V18_batch220lr0.005994840990751982.brainsave

It did terribly in Phase 2. I am trying phase 1 on V16 with a 0.1 and 0.01 leaky L1 clamp loss.
V16_leak0.1_batch250lr0.005994840990751982.brainsave
V16_leak0.01_batch370lr0.005994840990751982.brainsave

The leaky L1 clamp didn't help. The neural net is still only around 75% efficiency of the guide AI. Maybe I need to run it with the guide for around a million points before trying to pull it off the guide. Yes, I should go with the guide for a long time to see if the neural net is capable of coming closer to replicating it. One way to know for sure is to try phase 2 letting epsilon_guide increase or decrease.

Trying it with 100 000 guide points.

What I have seen interestingly is that epsilon_explore probably wants to increase further than it has. Also, epsilon guide goes from 0 to 1 at 600000 back to 0.1 at 9000 and ends at 0.2. Hard to understand it. Anyway, I think the most important thing to do is improve phase 1. One idea I had is to have a neural net with with one hidden layer to replicate the AI as per the normal method (outputs a one hot), then transfer the input and hidden layer into a neural net that outputs a quality. I could also try to change the slopes on L1 clamp loss. It's much worse for the winning direction to be too small than too large, so it really shouldn't be evenly sloped.